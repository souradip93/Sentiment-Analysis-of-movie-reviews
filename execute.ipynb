{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "execute.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souradip93/CS69002_9A_18CS60R07/blob/master/execute.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cZ6RDK_aJF8N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**PATHS**"
      ]
    },
    {
      "metadata": {
        "id": "0yqVo0ZwJLt8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_file_path = \"/content/drive/My Drive/Colab Notebooks/data/Test_5K.txt\"\n",
        "model_file_path = \"/content/drive/My Drive/Colab Notebooks/data/model_1d.mdl\"\n",
        "\n",
        "uni_vocab = \"/content/drive/My Drive/Colab Notebooks/data/word_to_ix.pickle\"\n",
        "bigram_vocab = '/content/drive/My Drive/Colab Notebooks/data/word_to_ix_bigram.pickle'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M35VUK3sWyiV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Neural network for Task 1**"
      ]
    },
    {
      "metadata": {
        "id": "644dEiuvcWgi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Task1(nn.Module):\n",
        "  def __init__(self, num_labels, hidden_layer_neurons, vocab_size):\n",
        "    super(Task1, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden_layer_neurons)\n",
        "    self.lin2 = nn.Linear(hidden_layer_neurons, num_labels)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    return F.softmax(self.lin2(out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "aq524mr7MoBE"
      },
      "cell_type": "markdown",
      "source": [
        "#**Neural network for Task 2**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ypC8WVQNMoBH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task2(nn.Module):\n",
        "  def __init__(self, num_labels, hidden_layer_neurons1, hidden_layer_neurons2, vocab_size):\n",
        "    super(Task2, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden_layer_neurons1)\n",
        "    self.lin2 = nn.Linear(hidden_layer_neurons1, hidden_layer_neurons2)\n",
        "    self.lin3 = nn.Linear(hidden_layer_neurons2, num_labels)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    return F.softmax(self.lin3(out))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "U1nvWbPu3fNt"
      },
      "cell_type": "markdown",
      "source": [
        "#**Neural network for Task 3**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "x5yGKC3k3fN1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task3(nn.Module):\n",
        "  def __init__(self, num_labels, hidden_layer_neurons1, hidden_layer_neurons2, hidden_layer_neurons3, vocab_size):\n",
        "    super(Task3, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden_layer_neurons1)\n",
        "    self.lin2 = nn.Linear(hidden_layer_neurons1, hidden_layer_neurons2)\n",
        "    self.lin3 = nn.Linear(hidden_layer_neurons2, hidden_layer_neurons3)\n",
        "    self.lin4 = nn.Linear(hidden_layer_neurons3, num_labels)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    out = self.lin3(out)\n",
        "    return F.softmax(self.lin4(out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4O3OPX1F-Okl"
      },
      "cell_type": "markdown",
      "source": [
        "#**Neural network for Task 4**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "H3k9RHcB-Okq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task4(nn.Module):\n",
        "  def __init__(self, num_labels, hidden_layer_neurons1, hidden_layer_neurons2, hidden_layer_neurons3, vocab_size):\n",
        "    super(Task4, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden_layer_neurons1)\n",
        "    torch.nn.Dropout(0.5)\n",
        "    self.lin2 = nn.Linear(hidden_layer_neurons1, hidden_layer_neurons2)\n",
        "    torch.nn.Dropout(0.5)\n",
        "    if hidden_layer_neurons3 is not None:\n",
        "      self.lin3 = nn.Linear(hidden_layer_neurons2, hidden_layer_neurons3)\n",
        "      torch.nn.Dropout(0.5)\n",
        "      self.lin4 = nn.Linear(hidden_layer_neurons3, num_labels)\n",
        "    else:\n",
        "      self.lin3 = nn.Linear(hidden_layer_neurons2, num_labels)\n",
        "      self.lin4 = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    out = self.lin3(out)\n",
        "    if self.lin4 is None:\n",
        "      return F.softmax(out)\n",
        "    else:\n",
        "      return F.softmax(self.lin4(out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "J72D9cOelDlZ"
      },
      "cell_type": "markdown",
      "source": [
        "#**Neural network for Task 5**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "e_X1DfQwlDla",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task5a(nn.Module):\n",
        "  def __init__(self, num_labels, hidden_layer_neurons1, hidden_layer_neurons2, hidden_layer_neurons3, vocab_size):\n",
        "    super(Task5a, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden_layer_neurons1)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    self.lin2 = nn.Linear(hidden_layer_neurons1, hidden_layer_neurons2)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    if hidden_layer_neurons3 is not None:\n",
        "      self.lin3 = nn.Linear(hidden_layer_neurons2, hidden_layer_neurons3)\n",
        "      torch.nn.Dropout(0.05)\n",
        "      self.lin4 = nn.Linear(hidden_layer_neurons3, num_labels)\n",
        "    else:\n",
        "      self.lin3 = nn.Linear(hidden_layer_neurons2, num_labels)\n",
        "      self.lin4 = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    out = self.lin3(out)\n",
        "    if self.lin4 is None:\n",
        "      return F.softmax(out)\n",
        "    else:\n",
        "      return F.softmax(self.lin4(out))\n",
        "    \n",
        "class Task5b(nn.Module):\n",
        "  def __init__(self, num_labels, hidden_layer_neurons1, hidden_layer_neurons2, hidden_layer_neurons3, vocab_size):\n",
        "    super(Task5b, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden_layer_neurons1)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    self.lin2 = nn.Linear(hidden_layer_neurons1, hidden_layer_neurons2)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    if hidden_layer_neurons3 is not None:\n",
        "      self.lin3 = nn.Linear(hidden_layer_neurons2, hidden_layer_neurons3)\n",
        "      torch.nn.Dropout(0.05)\n",
        "      self.lin4 = nn.Linear(hidden_layer_neurons3, num_labels)\n",
        "    else:\n",
        "      self.lin3 = nn.Linear(hidden_layer_neurons2, num_labels)\n",
        "      self.lin4 = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.tanh(self.lin2(out))\n",
        "    out = self.lin3(out)\n",
        "    if self.lin4 is None:\n",
        "      return F.softmax(out)\n",
        "    else:\n",
        "      return F.softmax(self.lin4(out))\n",
        "    \n",
        "class Task5c(nn.Module):\n",
        "  def __init__(self, num_labels, hidden_layer_neurons1, hidden_layer_neurons2, hidden_layer_neurons3, vocab_size):\n",
        "    super(Task5c, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden_layer_neurons1)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    self.lin2 = nn.Linear(hidden_layer_neurons1, hidden_layer_neurons2)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    if hidden_layer_neurons3 is not None:\n",
        "      self.lin3 = nn.Linear(hidden_layer_neurons2, hidden_layer_neurons3)\n",
        "      torch.nn.Dropout(0.05)\n",
        "      self.lin4 = nn.Linear(hidden_layer_neurons3, num_labels)\n",
        "    else:\n",
        "      self.lin3 = nn.Linear(hidden_layer_neurons2, num_labels)\n",
        "      self.lin4 = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.sigmoid(self.lin2(out))\n",
        "    out = self.lin3(out)\n",
        "    if self.lin4 is None:\n",
        "      return F.softmax(out)\n",
        "    else:\n",
        "      return F.softmax(self.lin4(out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6yFv7dE-RJuP"
      },
      "cell_type": "markdown",
      "source": [
        "#**Neural network for Task 6**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6uYRUPqLRJuQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task6(nn.Module):\n",
        "  def __init__(self, num_labels, hidden_layer_neurons1, hidden_layer_neurons2, hidden_layer_neurons3, vocab_size):\n",
        "    super(Task6, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden_layer_neurons1)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    self.lin2 = nn.Linear(hidden_layer_neurons1, hidden_layer_neurons2)\n",
        "    torch.nn.Dropout(0.05)\n",
        "    if hidden_layer_neurons3 is not None:\n",
        "      self.lin3 = nn.Linear(hidden_layer_neurons2, hidden_layer_neurons3)\n",
        "      torch.nn.Dropout(0.05)\n",
        "      self.lin4 = nn.Linear(hidden_layer_neurons3, num_labels)\n",
        "    else:\n",
        "      self.lin3 = nn.Linear(hidden_layer_neurons2, num_labels)\n",
        "      self.lin4 = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    out = self.lin3(out)\n",
        "    if self.lin4 is None:\n",
        "      return F.softmax(out)\n",
        "    else:\n",
        "      return F.softmax(self.lin4(out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uE09kr0dapm5"
      },
      "cell_type": "markdown",
      "source": [
        "#**Neural network for Task 7**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "duN32ZpRapm_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Task7(nn.Module):\n",
        "  def __init__(self, num_labels, hidden_layer_neurons1, hidden_layer_neurons2, hidden_layer_neurons3, vocab_size):\n",
        "    super(Task7, self).__init__()\n",
        "    self.lin1 = nn.Linear(vocab_size, hidden_layer_neurons1)\n",
        "    torch.nn.Dropout(0.5)\n",
        "    self.lin2 = nn.Linear(hidden_layer_neurons1, hidden_layer_neurons2)\n",
        "    torch.nn.Dropout(0.5)\n",
        "    if hidden_layer_neurons3 is not None:\n",
        "      self.lin3 = nn.Linear(hidden_layer_neurons2, hidden_layer_neurons3)\n",
        "      torch.nn.Dropout(0.5)\n",
        "      self.lin4 = nn.Linear(hidden_layer_neurons3, num_labels)\n",
        "    else:\n",
        "      self.lin3 = nn.Linear(hidden_layer_neurons2, num_labels)\n",
        "      self.lin4 = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.lin1(x)\n",
        "    out = F.relu(self.lin2(out))\n",
        "    out = self.lin3(out)\n",
        "    if self.lin4 is None:\n",
        "      return F.softmax(out)\n",
        "    else:\n",
        "      return F.softmax(self.lin4(out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rrbL7xu6nSjD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**TESTING**"
      ]
    },
    {
      "metadata": {
        "id": "dbUr9AhpaUPp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Make bow vector"
      ]
    },
    {
      "metadata": {
        "id": "5dmsDsJAaTFt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import time\n",
        "\n",
        "def make_bow_vector(sentence, word_to_ix):\n",
        "    # create a vector of zeros of vocab size = len(word_to_idx)\n",
        "    vec = torch.zeros(len(word_to_ix))\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            #raise ValueError('Word',word,' not present in the dictionary. Sorry!')\n",
        "            vec[ word_to_ix['UNKNOWN'] ] += 1\n",
        "        else:\n",
        "            vec[word_to_ix[word]]+=1\n",
        "    return vec.view(1, -1)\n",
        "  \n",
        "def make_bow_vector_bigram(sentence, word_to_ix_bigram):\n",
        "    # create a vector of zeros of vocab size = len(word_to_idx)\n",
        "    vec = torch.zeros(len(word_to_ix_bigram))\n",
        "    for ind,word1 in enumerate(sentence[:-1]):\n",
        "        word2 = sentence[ind+1]\n",
        "        if (word1, word2) not in word_to_ix_bigram:\n",
        "            #raise ValueError('Word',word,' not present in the dictionary. Sorry!')\n",
        "            #vec[ word_to_ix_bigram['UNKNOWN'] ] += 1\n",
        "            pass\n",
        "        else:\n",
        "            vec[word_to_ix_bigram[(word1, word2)]]+=1\n",
        "    return vec.view(1, -1)\n",
        "\n",
        "def make_target(label, label_to_ix):\n",
        "    #print(label)\n",
        "    return torch.LongTensor([label_to_ix[label]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WdrGFK35YqSA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "def calculateMetrics(task, test_data_X, word_to_ix, fname):\n",
        "  print('--- AFTER TRAINING ---')\n",
        "  tp = 0\n",
        "  tn = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "\n",
        "  for ind, instance in enumerate(test_data_X):\n",
        "      bow_vec = Variable(make_bow_vector(instance, word_to_ix))\n",
        "      logprobs = task(bow_vec)\n",
        "      pred = np.argmax(logprobs.cpu().data.numpy())\n",
        "      label = test_data_Y[ind]\n",
        "      if label == 1 and pred == 1:\n",
        "        tp += 1\n",
        "      elif label == 1 and pred == 0:\n",
        "        fn += 1\n",
        "      elif label == 0 and pred == 0:\n",
        "        tn += 1\n",
        "      else:\n",
        "        fp += 1\n",
        "\n",
        "  accuracy = float(tp+tn)/ len(test_data_X)\n",
        "  recall = float(tp)/ (tp+fn)\n",
        "  precision = float(tp)/ (tp+fp)\n",
        "  fmeasure = float(2*recall*precision) / (precision+recall)\n",
        "  \n",
        "  print('Accuracy - ' + str(accuracy))\n",
        "  print('Recall - ' + str(recall))\n",
        "  print('Precision - ' + str(precision))\n",
        "  print('Fmeasure - ' + str(fmeasure))\n",
        "  \n",
        "def calculateMetricsBigram(task, test_data_X, word_to_ix_bigram, fname):\n",
        "  print('--- AFTER TRAINING ---')\n",
        "  tp = 0\n",
        "  tn = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "\n",
        "  for ind, instance in enumerate(test_data_X):\n",
        "      bow_vec = Variable(make_bow_vector_bigram(instance, word_to_ix_bigram))\n",
        "      logprobs = task(bow_vec)\n",
        "      pred = np.argmax(logprobs.cpu().data.numpy())\n",
        "      label = test_data_Y[ind]\n",
        "      if label == 1 and ix_to_label[pred] == 1:\n",
        "        tp += 1\n",
        "      elif label == 1 and ix_to_label[pred] == 0:\n",
        "        fn += 1\n",
        "      elif label == 0 and ix_to_label[pred] == 0:\n",
        "        tn += 1\n",
        "      else:\n",
        "        fp += 1\n",
        "\n",
        "  accuracy = float(tp+tn)/ len(test_data_X)\n",
        "  recall = float(tp)/ (tp+fn)\n",
        "  precision = float(tp)/ (tp+fp)\n",
        "  fmeasure = float(2*recall*precision) / (precision+recall)\n",
        "  \n",
        "  print('Accuracy - ' + str(accuracy))\n",
        "  print('Recall - ' + str(recall))\n",
        "  print('Precision - ' + str(precision))\n",
        "  print('Fmeasure - ' + str(fmeasure))\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1cYXFJtETb0a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "metadata": {
        "id": "kECKJFLbTrD-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Remove br tags and numbers"
      ]
    },
    {
      "metadata": {
        "id": "akSG_F8yWKh_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_br_tags_and_numbers(train_data_X):\n",
        "  regex = re.compile(r'<.*?>|\\d+')\n",
        "  train_data_X = [regex.sub(' ', x) for x in train_data_X]\n",
        "  train_data_X = [x.replace(\"'\", '').replace('\\n\\n', ' ') for x in train_data_X]\n",
        "  return train_data_X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fhOfwbIkTzUn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. Tokenize - Consider only alphanumeric characters"
      ]
    },
    {
      "metadata": {
        "id": "-8dXaj3SQyWO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "  \n",
        "def tokenize(train_data_X):\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  train_data_X = [tokenizer.tokenize(x) for x in train_data_X]\n",
        "  #train_data_X = [x.split(' ') for x in train_data_X]\n",
        "  #train_data_X = [[y for y in x if y != ''] for x in train_data_X]\n",
        "  #train_data_X[0], train_data_Y[0]\n",
        "  return train_data_X\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J-158l3wT9D_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. Convert to lower case"
      ]
    },
    {
      "metadata": {
        "id": "lLLIjYb5Qc9_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_to_lower(train_data_X):\n",
        "  train_data_X = [[y.lower() for y in x] for x in train_data_X]\n",
        "  #train_data_X[0], train_data_Y[0]\n",
        "  return train_data_X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1d2fjmatUAqV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "4. Remove stopwords"
      ]
    },
    {
      "metadata": {
        "id": "i0bZVBGlX5TF",
        "colab_type": "code",
        "outputId": "879572aa-c686-4715-af9b-683cf89d1c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(train_data_X):\n",
        "  stopword_set = set(stopwords.words('english'))\n",
        "  train_data_X = [[y for y in x if y not in stopword_set] for x in train_data_X]\n",
        "  #train_data_X[0], train_data_Y[0]\n",
        "  return train_data_X"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iM9RmpYmUDXC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "5. Perform lemmatization"
      ]
    },
    {
      "metadata": {
        "id": "Zc3IRBb5bNo4",
        "colab_type": "code",
        "outputId": "4cc59dd2-e24b-4996-837e-8c2e8235177e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')\n",
        "\n",
        "def lemmatize(train_data_X):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  train_data_X = [[lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(y,'n'), 'v'), 'a'), 'r') for y in x] for x in train_data_X]\n",
        "  return train_data_X"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3ek7Ra2KZXb3"
      },
      "cell_type": "markdown",
      "source": [
        "6. Remove punctuation"
      ]
    },
    {
      "metadata": {
        "id": "3Bh0ZpU-rY8L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "def remove_punctuation(train_data_X):\n",
        "  table = str.maketrans(' ', ' ', string.punctuation)\n",
        "  train_data_X = [x.translate(table) for x in train_data_X]\n",
        "  return train_data_X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fip3k6R2VAOS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Preprocessing on Test data**"
      ]
    },
    {
      "metadata": {
        "id": "B7FsSfNdZRZm",
        "colab_type": "code",
        "outputId": "85767af7-79d0-44ce-9477-094e504ef185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "df_test = pd.read_csv(test_file_path, sep='\\t')\n",
        "#df_test.head()\n",
        "\n",
        "test_data_X = df_test['text'].astype(str).tolist()\n",
        "test_data_Y = df_test['label'].astype(int)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "58o-WKWaTBnL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_data_X = remove_br_tags_and_numbers(test_data_X)\n",
        "#test_data_X = remove_punctuation(test_data_X)\n",
        "test_data_X = tokenize(test_data_X)\n",
        "test_data_X = convert_to_lower(test_data_X)\n",
        "test_data_X = remove_stopwords(test_data_X)\n",
        "test_data_X = lemmatize(test_data_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZQiIf7c4_jVk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Load Vocab**"
      ]
    },
    {
      "metadata": {
        "id": "AeGRHDybXVIH",
        "colab_type": "code",
        "outputId": "12fee96b-1d90-4878-9157-781414b49517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "word_to_ix_path = uni_vocab\n",
        "word_to_ix = pickle.load(open(word_to_ix_path, 'rb'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JVRa44hZ_mZu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Load Bigram Vocab**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aX-vJFf__y_9",
        "outputId": "bf7bb89e-4f68-44f4-e0b8-831f5dadd11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "word_to_ix_path = bigram_vocab\n",
        "word_to_ix_bigram = pickle.load(open(word_to_ix_path, 'rb'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QHxGx0FS_3QT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Load Model**"
      ]
    },
    {
      "metadata": {
        "id": "C5tLQ-kQksRk",
        "colab_type": "code",
        "outputId": "94a4fddd-5083-48aa-c386-4972a1135ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_path = model_file_path\n",
        "bow = torch.load(model_path)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0-vkLqHTXWzg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "calculateMetrics(bow, test_data_X, word_to_ix, '/content/drive/My Drive/Colab Notebooks/data/model1d_pred.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AvKSdlj9A6tY",
        "outputId": "bb18071e-a092-4420-a67f-6b0cab6a8582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "calculateMetricsBigram(bow, test_data_X, word_to_ix_bigram, '/content/drive/My Drive/Colab Notebooks/data/model6_pred.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- AFTER TRAINING ---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}