{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SciBERT_RelationExtraction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souradip93/CS69002_9A_18CS60R07/blob/master/SciBERT_RelationExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxttho3ANT3O",
        "colab_type": "code",
        "outputId": "4b4529f2-c1b5-4af1-c6cb-47fb78eab4b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install jsonlines"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.3)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.156)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.156 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.156)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.156->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.156->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.156->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rbLcwJFNcai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import logging\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "import jsonlines\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z8MH6WWwr25",
        "colab_type": "text"
      },
      "source": [
        "Get dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZeF26B5woMX",
        "colab_type": "code",
        "outputId": "12ba0134-d2c5-4964-ec09-c0b30a4e0617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/allenai/scibert/master/data/text_classification/sciie-relation-extraction/train.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-31 14:58:42--  https://raw.githubusercontent.com/allenai/scibert/master/data/text_classification/sciie-relation-extraction/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 823836 (805K) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "\rtrain.txt             0%[                    ]       0  --.-KB/s               \rtrain.txt           100%[===================>] 804.53K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-05-31 14:58:42 (32.5 MB/s) - ‘train.txt’ saved [823836/823836]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4ncRSx8BAEl",
        "colab_type": "text"
      },
      "source": [
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids:   0   0  0    0    0     0      0   0    1  1  1   1  1   1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids:   0   0   0   0  0     0   0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ9X10fQA7jv",
        "colab_type": "text"
      },
      "source": [
        "BERT TOKENIZER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz7cgFr8NngQ",
        "colab_type": "code",
        "outputId": "6c778f26-5e63-4d0d-9713-bc42dd6c614c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('scibert_scivocab_uncased/vocab.txt', do_lower_case=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file scibert_scivocab_uncased/vocab.txt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJdS62ngzHpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_(file_path):\n",
        "  with jsonlines.open(file_path) as f_in:\n",
        "    text_id = 0\n",
        "    train_data = []\n",
        "    for json_object in f_in:\n",
        "        train_data.append(convert_text_to_features(\n",
        "            text=json_object.get('text'),\n",
        "            seq_length=120,\n",
        "            label=json_object.get('label'),\n",
        "            text_unique_id=text_id\n",
        "        ))\n",
        "        text_id += 1\n",
        "  return train_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vbhdEYkBN0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_text_to_features(text, seq_length, label, text_unique_id):\n",
        "    \"\"\"Loads a data file into a list of `InputFeature`s.\"\"\"\n",
        "\n",
        "    feature = {}\n",
        "    \n",
        "    tokens_a = tokenizer.tokenize(text)\n",
        "\n",
        "    if len(tokens_a) > seq_length - 2:\n",
        "      tokens_a = tokens_a[0:(seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    input_type_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    input_type_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "      tokens.append(token)\n",
        "      input_type_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    input_type_ids.append(0)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < seq_length:\n",
        "      input_ids.append(0)\n",
        "      input_mask.append(0)\n",
        "      input_type_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == seq_length\n",
        "    assert len(input_mask) == seq_length\n",
        "    assert len(input_type_ids) == seq_length\n",
        "\n",
        "    feature['unique_id'] = text_unique_id\n",
        "    feature['tokens'] = tokens\n",
        "    feature['input_ids'] = input_ids\n",
        "    feature['input_mask'] = input_mask\n",
        "    feature['input_type_ids'] = input_type_ids\n",
        "        \n",
        "    return feature,label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhUntsi3zerV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = read_('train.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWY3yQSsMFtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_list = []\n",
        "for _,label in train_data:\n",
        "  if label not in labels_list:\n",
        "    labels_list.append(label)\n",
        "labels_list\n",
        "\n",
        "def get_label_index(label):\n",
        "  return labels_list.index(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2FtQSqzDPrG",
        "colab_type": "code",
        "outputId": "2c9403be-2cad-4a43-fff0-3e7886c104bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5290
        }
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "#model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('scibert_scivocab_uncased/weights.tar.gz')\n",
        "model.to(device)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.modeling:loading archive file scibert_scivocab_uncased/weights.tar.gz\n",
            "INFO:pytorch_pretrained_bert.modeling:extracting archive file scibert_scivocab_uncased/weights.tar.gz to temp dir /tmp/tmpfbobjgma\n",
            "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): BertLayerNorm()\n",
              "    (dropout): Dropout(p=0.1)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEwcXLSTDgNn",
        "colab_type": "code",
        "outputId": "2866e5b9-ded1-4fbf-e1e2-becce5f5e19c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_input_ids = torch.tensor([feature['input_ids'] for feature,_ in train_data], dtype=torch.long).to(device)\n",
        "all_input_mask = torch.tensor([feature['input_mask'] for feature,_ in train_data], dtype=torch.long).to(device)\n",
        "all_input_type_ids = torch.tensor([feature['input_type_ids'] for feature,_ in train_data], dtype=torch.long).to(device)\n",
        "all_labels = torch.tensor([get_label_index(label) for _,label in train_data], dtype=torch.long).to(device)\n",
        "all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long).to(device)\n",
        "\n",
        "all_input_ids.size()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3219, 120])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmZQ5RXOEH_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_data = TensorDataset(all_input_ids, all_input_mask, all_input_type_ids, all_labels, all_example_index)\n",
        "eval_sampler = SequentialSampler(eval_data)\n",
        "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt5Al8odF9uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertEmbedder(torch.nn.Module):\n",
        "  def __init__(self, bert_model):\n",
        "    super(BertEmbedder, self).__init__()\n",
        "    \n",
        "    self.bert_model = bert_model\n",
        "    self.output_dim = bert_model.config.hidden_size\n",
        "    \n",
        "  def forward(self, input_ids, input_mask, token_type_ids):\n",
        "    all_encoder_layers,_ = self.bert_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=input_mask)\n",
        "    all_encoder_layers = torch.stack(all_encoder_layers)\n",
        "    embedding = all_encoder_layers[-1]\n",
        "    return embedding\n",
        "  \n",
        "  def get_output_dim(self):\n",
        "    return self.output_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xJFrnhxOiok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertForClassification(torch.nn.Module):\n",
        "  def __init__(self, text_embedder, dropout: float = 0.2):\n",
        "        \n",
        "    super(BertForClassification, self).__init__()\n",
        "    self.text_embedder = text_embedder\n",
        "    self.dropout = torch.nn.Dropout(dropout)\n",
        "    self.num_classes = len(labels_list)\n",
        "    self.classifier_feedforward = torch.nn.Linear(self.text_embedder.get_output_dim()  , self.num_classes).to(device)\n",
        "    self.loss = torch.nn.CrossEntropyLoss()\n",
        "               \n",
        "               \n",
        "  def forward(self, input_ids, input_mask, input_type_ids):\n",
        "    embedded_text = self.text_embedder(input_ids, input_mask, input_type_ids)\n",
        "    pooled = self.dropout(embedded_text[:, 0, :])\n",
        "    logits = self.classifier_feedforward(pooled)\n",
        "    #class_probs = F.softmax(logits, dim=1)\n",
        "    \n",
        "    #output_dict = {\"logits\": logits}\n",
        "    #output_dict[\"class_probs\"] = class_probs\n",
        "    #loss = self.loss(class_probs, label)\n",
        "    #output_dict[\"loss\"] = loss\n",
        "    #return output_dict\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecmotW_A0J1A",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw8supfCJ2RA",
        "colab_type": "code",
        "outputId": "1b5958b0-6ba1-4c47-ce80-db2a2f86411a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bert_embedder = BertEmbedder(model)\n",
        "\n",
        "for input_ids, input_mask, input_type_ids, labels, example_indices in eval_dataloader:\n",
        "  embedding = bert_embedder(input_ids, input_mask, input_type_ids)\n",
        "  #print(embedding.size())\n",
        "  \n",
        "embedding[-1].size()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([120, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEJgeAbP7jks",
        "colab_type": "code",
        "outputId": "eb582b05-59a1-448d-ac79-4b1d4899f859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(torch.cuda.get_device_properties(device).total_memory)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15812263936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YY2K7WRlPHu",
        "colab_type": "code",
        "outputId": "5118dc3f-79f9-464e-d174-3013ba622eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#bert_embedder = BertEmbedder(model)\n",
        "#bert_classifier = BertForClassification(bert_embedder)\n",
        "\n",
        "#param_optimizer = list(model.named_parameters())\n",
        "#no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "#optimizer_grouped_parameters = [\n",
        "#    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "#    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "#    ]\n",
        "#optimizer = torch.optim.Adam(optimizer_grouped_parameters, lr=0.0001)\n",
        "#optimizer.zero_grad()\n",
        "\n",
        "for eopch in range(15):\n",
        "  tr_loss = 0\n",
        "  correct_count,total_count = 0,0\n",
        "  for input_ids, input_mask, input_type_ids, labels, example_indices in eval_dataloader:\n",
        "    logits = bert_classifier(input_ids, input_mask, input_type_ids)\n",
        "    #loss = output_dict['loss']\n",
        "    loss = bert_classifier.loss(logits, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    tr_loss += loss.item()\n",
        "    \n",
        "    #num_classes = output_dict['class_probs'].size(-1)\n",
        "    #predictions = output_dict['class_probs'].view((-1, num_classes))\n",
        "    \n",
        "    num_classes = logits.size(-1)\n",
        "    predictions = logits.view((-1, num_classes))\n",
        "    gold_labels = labels.view(-1).long()\n",
        "    \n",
        "    predictions = predictions.view((-1, num_classes))\n",
        "    gold_labels = gold_labels.view(-1).long()\n",
        "    \n",
        "    top_k = predictions.max(-1)[1].unsqueeze(-1)\n",
        "    correct = top_k.eq(gold_labels.unsqueeze(-1)).float()\n",
        "    correct_count += correct.sum()\n",
        "    total_count += gold_labels.numel()\n",
        "    \n",
        "  accuracy = float(correct_count) / float(total_count)\n",
        "\n",
        "  print(tr_loss,accuracy)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40.86948500573635 0.932587760173967\n",
            "34.73544389009476 0.9425287356321839\n",
            "35.758873203769326 0.9415967691829761\n",
            "33.069150384515524 0.949363156259708\n",
            "28.060105085372925 0.95278036657347\n",
            "26.336552642285824 0.9577508543025784\n",
            "25.135287575423717 0.9589934762348555\n",
            "24.858046356588602 0.9648959304131718\n",
            "40.59861145541072 0.9338303821062441\n",
            "37.712253373116255 0.9369369369369369\n",
            "34.879836241714656 0.9465672569120845\n",
            "28.294928221032023 0.9540229885057471\n",
            "22.955023393034935 0.961789375582479\n",
            "26.70872782636434 0.9596147872009941\n",
            "24.42994167841971 0.958372165268717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lreou1kAcHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "9fb9110f-0e70-4c84-8feb-d1b96744c6cc"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/allenai/scibert/master/data/text_classification/sciie-relation-extraction/test.txt"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-31 17:56:26--  https://raw.githubusercontent.com/allenai/scibert/master/data/text_classification/sciie-relation-extraction/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 249465 (244K) [text/plain]\n",
            "Saving to: ‘test.txt.1’\n",
            "\n",
            "test.txt.1          100%[===================>] 243.62K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-05-31 17:56:27 (14.4 MB/s) - ‘test.txt.1’ saved [249465/249465]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xas2TtMwAi-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = read_('test.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kORUZHbhArrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_list = []\n",
        "for _,label in test_data:\n",
        "  if label not in labels_list:\n",
        "    labels_list.append(label)\n",
        "labels_list\n",
        "\n",
        "def get_label_index(label):\n",
        "  return labels_list.index(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwTmsCiQA0Aw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "943254ce-c274-424b-a9e7-44a10ed542c3"
      },
      "source": [
        "all_input_ids = torch.tensor([feature['input_ids'] for feature,_ in test_data], dtype=torch.long).to(device)\n",
        "all_input_mask = torch.tensor([feature['input_mask'] for feature,_ in test_data], dtype=torch.long).to(device)\n",
        "all_input_type_ids = torch.tensor([feature['input_type_ids'] for feature,_ in test_data], dtype=torch.long).to(device)\n",
        "all_labels = torch.tensor([get_label_index(label) for _,label in test_data], dtype=torch.long).to(device)\n",
        "all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long).to(device)\n",
        "\n",
        "all_input_ids.size()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([974, 120])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiMNvvkyBHqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_data = TensorDataset(all_input_ids, all_input_mask, all_input_type_ids, all_labels, all_example_index)\n",
        "eval_sampler = SequentialSampler(eval_data)\n",
        "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt3iVNYnBUdA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7745d244-c9aa-4c18-c32e-d355e2e29019"
      },
      "source": [
        "for input_ids, input_mask, input_type_ids, labels, example_indices in eval_dataloader:\n",
        "  logits = bert_classifier(input_ids, input_mask, input_type_ids)\n",
        "  class_probs = F.softmax(logits, dim=1)\n",
        "\n",
        "  num_classes = class_probs.size(-1)\n",
        "  predictions = class_probs.view((-1, num_classes))\n",
        "  gold_labels = labels.view(-1).long()\n",
        "\n",
        "  predictions = predictions.view((-1, num_classes))\n",
        "  gold_labels = gold_labels.view(-1).long()\n",
        "\n",
        "  top_k = predictions.max(-1)[1].unsqueeze(-1)\n",
        "  correct = top_k.eq(gold_labels.unsqueeze(-1)).float()\n",
        "  correct_count += correct.sum()\n",
        "  total_count += gold_labels.numel()\n",
        "    \n",
        "accuracy = float(correct_count) / float(total_count)\n",
        "print(accuracy)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7693775339852135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7hKXQxHxiPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "ab43149e-fdfa-4d78-b36c-928772176f1d"
      },
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_uncased.tar"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-31 16:43:38--  https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_uncased.tar\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.217.136\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.217.136|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 410593280 (392M) [application/x-tar]\n",
            "Saving to: ‘scibert_scivocab_uncased.tar’\n",
            "\n",
            "scibert_scivocab_un 100%[===================>] 391.57M  20.2MB/s    in 20s     \n",
            "\n",
            "2019-05-31 16:43:59 (19.6 MB/s) - ‘scibert_scivocab_uncased.tar’ saved [410593280/410593280]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiFeIPX9xqsQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c539c4d1-cf34-4326-92dd-c72dae04cf8e"
      },
      "source": [
        "!tar -xvf scibert_scivocab_uncased.tar"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scibert_scivocab_uncased/\n",
            "scibert_scivocab_uncased/weights.tar.gz\n",
            "scibert_scivocab_uncased/vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgfd54jGwxqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5d542572-24e1-47ff-c4d6-7bb656be53a9"
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained('scibert_scivocab_uncased/vocab.txt', do_lower_case=True)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file scibert_scivocab_uncased/vocab.txt\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}